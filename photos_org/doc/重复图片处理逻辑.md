
## **重复图片处理逻辑完整设计文档**

### **1. 核心原则**

- **基于内容哈希值判断重复**：相同哈希值 = 相同文件内容
- **避免重复存储**：相同内容不重复存储物理文件
- **确保数据一致性**：数据库记录与物理文件保持同步
- **优化用户体验**：显示"成功"而不是"失败"
- **彻底清理孤儿记录**：确保数据完整性

### **2. 重复检查的四种情况**

```python
def _check_duplicate_file(self, file_hash: str, db_session) -> Dict:
    """
    检查文件是否重复的完整逻辑
    """
    # 情况1：数据库有记录 + 物理文件存在 = 完全重复
    existing_photo = db_session.query(Photo).filter(Photo.file_hash == file_hash).first()
    if existing_photo and existing_photo.original_path and Path(existing_photo.original_path).exists():
        # 检查智能处理状态
        if existing_photo.status == 'completed':
            return {
                "is_duplicate": True,
                "message": "文件已存在且已完成智能处理",
                "duplicate_type": "full_duplicate_completed"
            }
        elif existing_photo.status in ['imported', 'analyzing', 'error']:
            return {
                "is_duplicate": True,
                "message": "文件已存在但未完成智能处理",
                "duplicate_type": "full_duplicate_incomplete",
                "existing_photo": existing_photo
            }
    
    # 情况2：数据库有记录 + 物理文件不存在 = 孤儿记录
    if existing_photo:
        db_session.delete(existing_photo)
        db_session.commit()
        return {
            "is_duplicate": False,
            "message": "孤儿记录已清理",
            "duplicate_type": "orphan_cleaned"
        }
    
    # 情况3：数据库无记录 + 物理文件存在 = 物理重复
    existing_file_path = self._find_existing_file_by_hash(file_hash)
    if existing_file_path:
        return {
            "is_duplicate": True,
            "message": "文件已存在（重复）",
            "existing_path": existing_file_path,
            "duplicate_type": "physical_only"
        }
    
    # 情况4：数据库无记录 + 物理文件不存在 = 全新文件
    return {
        "is_duplicate": False,
        "message": "全新文件",
        "duplicate_type": "new_file"
    }
```

### **3. 各种情况的详细处理方法**

#### **3.1 完全重复且已完成智能处理**
```python
def _handle_full_duplicate_completed(self, duplicate_result: Dict) -> Tuple[bool, str, Optional[Dict], Optional[Dict]]:
    """处理已完成智能处理的完全重复文件"""
    return False, duplicate_result['message'], None, None
```

#### **3.2 完全重复但未完成智能处理**
```python
def _handle_full_duplicate_incomplete(self, duplicate_result: Dict) -> Tuple[bool, str, Optional[Dict], Optional[Dict]]:
    """处理未完成智能处理的完全重复文件"""
    existing_photo = duplicate_result['existing_photo']
    
    if existing_photo.status == 'error':
        # 错误状态，重新开始处理
        existing_photo.status = 'imported'
        db_session.commit()
        return True, "文件已存在，重新开始智能处理", None, None
    elif existing_photo.status == 'imported':
        # 已导入但未处理，继续处理
        return True, "文件已存在，继续智能处理", None, None
    elif existing_photo.status == 'analyzing':
        # 正在处理中，提供选项
        return False, "文件正在处理中，请稍候或选择强制重新处理", None, {
            'force_retry': True,
            'current_status': 'analyzing'
        }
    
    return False, "未知状态", None, None
```

#### **3.3 孤儿记录处理**
```python
def _handle_orphan_cleaned(self, duplicate_result: Dict, file_path: str, file_hash: str) -> Tuple[bool, str, Optional[Dict], Optional[Dict]]:
    """处理孤儿记录清理后的文件"""
    print(f"孤儿记录已清理: {duplicate_result['message']}")
    
    # 检查是否有对应的缩略图需要清理
    thumbnail_path = self._get_thumbnail_path_by_hash(file_hash)
    if thumbnail_path and Path(thumbnail_path).exists():
        Path(thumbnail_path).unlink()
        print(f"清理孤儿缩略图: {thumbnail_path}")
    
    # 清理孤儿记录的分析结果
    self._cleanup_orphan_analysis_results(file_hash)
    
    # 继续正常处理流程
    return self._handle_new_file(file_path, file_hash, move_file=True)

def _cleanup_orphan_analysis_results(self, file_hash: str):
    """清理孤儿记录的分析结果"""
    try:
        # 清理AI分析结果
        analysis_results = db_session.query(PhotoAnalysis).filter(
            PhotoAnalysis.photo_id.in_(
                db_session.query(Photo.id).filter(Photo.file_hash == file_hash)
            )
        ).all()
        for result in analysis_results:
            db_session.delete(result)
        
        # 清理质量评估结果
        quality_results = db_session.query(PhotoQuality).filter(
            PhotoQuality.photo_id.in_(
                db_session.query(Photo.id).filter(Photo.file_hash == file_hash)
            )
        ).all()
        for result in quality_results:
            db_session.delete(result)
        
        # 清理分类结果
        classification_results = db_session.query(PhotoClassification).filter(
            PhotoClassification.photo_id.in_(
                db_session.query(Photo.id).filter(Photo.file_hash == file_hash)
            )
        ).all()
        for result in classification_results:
            db_session.delete(result)
        
        db_session.commit()
        print(f"清理孤儿分析结果完成: {file_hash}")
        
    except Exception as e:
        print(f"清理孤儿分析结果失败: {e}")
        db_session.rollback()
```

#### **3.4 物理重复处理**
```python
def _handle_physical_duplicate(self, duplicate_result: Dict, file_path: str, file_hash: str) -> Tuple[bool, str, Optional[Dict], Optional[Dict]]:
    """处理物理重复的文件"""
    existing_path = duplicate_result['existing_path']
    
    # 检查缩略图是否已存在
    existing_thumbnail = self._check_thumbnail_by_hash(file_hash)
    if existing_thumbnail:
        thumbnail_path = existing_thumbnail
    else:
        thumbnail_path = self.generate_thumbnail(existing_path)
    
    # 提取元数据
    exif_data = self.extract_exif_data(existing_path)
    
    # 创建数据库记录
    photo_data = {
        'filename': Path(file_path).name,
        'original_path': existing_path,
        'file_hash': file_hash,
        'file_size': Path(existing_path).stat().st_size,
        'status': 'imported',
        'thumbnail_path': thumbnail_path,
        **exif_data
    }
    
    return True, "文件已存在，使用现有文件", photo_data, None
```

#### **3.5 全新文件处理**
```python
def _handle_new_file(self, file_path: str, file_hash: str, move_file: bool) -> Tuple[bool, str, Optional[Dict], Optional[Dict]]:
    """处理全新文件"""
    try:
        # 存储文件
        if move_file:
            storage_path = self.move_to_storage(file_path, file_hash)
        else:
            storage_path = self.copy_to_storage(file_path, file_hash)
        
        # 生成缩略图
        thumbnail_path = self.generate_thumbnail(storage_path)
        
        # 提取元数据
        exif_data = self.extract_exif_data(storage_path)
        
        # 创建数据库记录
        photo_data = {
            'filename': Path(file_path).name,
            'original_path': storage_path,
            'file_hash': file_hash,
            'file_size': Path(file_path).stat().st_size,
            'status': 'imported',
            'thumbnail_path': thumbnail_path,
            **exif_data
        }
        
        return True, "文件导入成功", photo_data, None
        
    except Exception as e:
        print(f"处理全新文件失败: {e}")
        return False, f"文件处理失败: {str(e)}", None, None
```

### **4. 辅助方法实现**

#### **4.1 物理文件查找**
```python
def _find_existing_file_by_hash(self, target_hash: str) -> Optional[str]:
    """在物理存储中查找具有相同哈希值的文件"""
    originals_dir = Path("photos_storage/originals")
    
    if not originals_dir.exists():
        return None
    
    for file_path in originals_dir.rglob('*'):
        if file_path.is_file():
            try:
                file_hash = self.calculate_file_hash(str(file_path))
                if file_hash == target_hash:
                    return str(file_path)
            except Exception as e:
                print(f"计算文件哈希失败 {file_path}: {e}")
                continue
    
    return None
```

#### **4.2 缩略图检查**
```python
def _check_thumbnail_by_hash(self, file_hash: str) -> Optional[str]:
    """基于哈希值检查缩略图是否存在"""
    thumbnail_path = f"photos_storage/thumbnails/{file_hash}_thumb.jpg"
    if Path(thumbnail_path).exists():
        return thumbnail_path
    return None

def _get_thumbnail_path_by_hash(self, file_hash: str) -> str:
    """根据文件哈希值生成缩略图路径"""
    return f"photos_storage/thumbnails/{file_hash}_thumb.jpg"
```

#### **4.3 文件哈希计算**
```python
def calculate_file_hash(self, file_path: str) -> str:
    """计算文件的MD5哈希值"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()
```

### **5. 完整的处理流程**

```python
def process_single_photo(self, file_path: str, move_file: bool = True, db_session=None):
    """处理单张照片的完整流程"""
    
    try:
        # 1. 文件验证
        if not self._validate_file(file_path):
            return False, "文件格式不支持", None, None
        
        # 2. 计算文件哈希
        file_hash = self.calculate_file_hash(file_path)
        
        # 3. 重复检查
        if db_session:
            duplicate_result = self._check_duplicate_file(file_hash, db_session)
            
            # 情况1.1：完全重复且已完成智能处理 - 跳过所有处理
            if duplicate_result['is_duplicate'] and duplicate_result.get('duplicate_type') == 'full_duplicate_completed':
                return False, duplicate_result['message'], None, None
            
            # 情况1.2：完全重复但未完成智能处理 - 继续智能处理
            elif duplicate_result['is_duplicate'] and duplicate_result.get('duplicate_type') == 'full_duplicate_incomplete':
                return self._handle_full_duplicate_incomplete(duplicate_result)
            
            # 情况2：孤儿记录 - 清理后继续正常处理
            elif not duplicate_result['is_duplicate'] and duplicate_result.get('duplicate_type') == 'orphan_cleaned':
                return self._handle_orphan_cleaned(duplicate_result, file_path, file_hash)
            
            # 情况3：物理重复 - 使用现有文件，继续处理
            elif duplicate_result['is_duplicate'] and duplicate_result.get('duplicate_type') == 'physical_only':
                return self._handle_physical_duplicate(duplicate_result, file_path, file_hash)
            
            # 情况4：全新文件 - 正常处理
            elif not duplicate_result['is_duplicate'] and duplicate_result.get('duplicate_type') == 'new_file':
                pass  # 继续正常处理流程
        
        # 4. 正常处理流程（适用于情况2和情况4）
        return self._handle_new_file(file_path, file_hash, move_file)
        
    except Exception as e:
        print(f"处理照片时发生错误: {e}")
        return False, f"处理失败: {str(e)}", None, None
```

### **6. 处理流程图（完整版）**

```
开始导入文件
    ↓
文件验证
    ↓
计算文件哈希
    ↓
重复检查
    ↓
┌─────────────────────────────────────────────────────────────┐
│ 情况1.1：完全重复且已完成智能处理                            │
│ 数据库有记录 + 物理文件存在 + status='completed'            │
│ → 跳过所有处理，返回失败                                    │
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│ 情况1.2：完全重复但未完成智能处理                            │
│ 数据库有记录 + 物理文件存在 + status in ['imported', 'analyzing', 'error'] │
│ → 继续智能处理流程                                          │
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│ 情况2：孤儿记录                                              │
│ 数据库有记录 + 物理文件不存在                                │
│ → 清理数据库记录、缩略图、分析结果，继续正常处理流程        │
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│ 情况3：物理重复                                              │
│ 数据库无记录 + 物理文件存在                                  │
│ → 使用现有文件，生成缩略图，创建数据库记录                  │
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│ 情况4：全新文件                                              │
│ 数据库无记录 + 物理文件不存在                                │
│ → 正常处理：存储文件、生成缩略图、创建数据库记录            │
└─────────────────────────────────────────────────────────────┘
    ↓
完成处理
```

### **7. 关键点总结**

1. **完全重复且已完成**：跳过所有处理，避免重复工作
2. **完全重复但未完成**：继续智能处理，确保数据完整性
3. **孤儿记录**：彻底清理，包括数据库记录、缩略图、分析结果等
4. **物理重复**：使用现有文件，但需要生成缩略图和创建数据库记录
5. **全新文件**：完整的导入流程
6. **错误处理**：完善的异常处理机制
7. **缩略图优化**：避免重复生成缩略图
8. **数据一致性**：确保数据库和物理文件同步

### **8. 注意事项**

- **哈希值计算**：确保哈希值计算的准确性
- **路径处理**：注意Windows/Unix路径分隔符的兼容性
- **文件权限**：确保有权限访问和操作文件
- **性能优化**：大量文件时考虑异步处理
- **日志记录**：详细记录处理过程和错误信息
- **事务管理**：确保数据库操作的原子性

这个完整的逻辑设计确保了重复文件处理的完整性、效率和可靠性。