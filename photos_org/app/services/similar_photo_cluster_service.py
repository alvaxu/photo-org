"""
ç›¸ä¼¼ç…§ç‰‡èšç±»æœåŠ¡æ¨¡å—

## åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. åŸºäºDBSCANç®—æ³•çš„ç›¸ä¼¼ç…§ç‰‡èšç±»
2. ä½¿ç”¨ResNet50ç‰¹å¾å‘é‡è¿›è¡Œèšç±»
3. æ”¯æŒå¯é…ç½®çš„èšç±»å‚æ•°
4. ä¸ç°æœ‰åˆ†ææµç¨‹é›†æˆ

## ä¸å…¶ä»–ç‰ˆæœ¬çš„ä¸åŒç‚¹ï¼š
- å‚è€ƒäººè„¸èšç±»æœåŠ¡çš„æ¶æ„è®¾è®¡
- ä½¿ç”¨ç‰¹å¾å‘é‡ï¼ˆimage_featuresï¼‰è¿›è¡Œèšç±»
- ä¸éœ€è¦ä»£è¡¨äººè„¸é€‰æ‹©é€»è¾‘
"""

import asyncio
import logging
from typing import List, Dict, Optional
from datetime import datetime
import json
import uuid  # ğŸ”¥ æ·»åŠ UUIDç”¨äºç”Ÿæˆå”¯ä¸€ID

# å»¶è¿Ÿå¯¼å…¥é‡å‹åº“
np = None
DBSCAN = None

def _lazy_import_dependencies():
    """å»¶è¿Ÿå¯¼å…¥é‡å‹åº“"""
    global np, DBSCAN
    
    if np is None:
        try:
            import numpy as np
            from sklearn.cluster import DBSCAN
            logging.info("æˆåŠŸåŠ è½½ç›¸ä¼¼ç…§ç‰‡èšç±»ä¾èµ–åº“")
        except ImportError as e:
            logging.error(f"èšç±»ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")

from app.core.config import settings
from app.db.session import get_db
from app.models.photo import Photo, DuplicateGroup, DuplicateGroupPhoto
from sqlalchemy.orm import Session
from sqlalchemy import func

logger = logging.getLogger(__name__)

# å…¨å±€èšç±»ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
cluster_task_status = {}

class SimilarPhotoClusterService:
    """ç›¸ä¼¼ç…§ç‰‡èšç±»æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç›¸ä¼¼ç…§ç‰‡èšç±»æœåŠ¡"""
        pass
    
    @property
    def similarity_threshold(self) -> float:
        """
        åŠ¨æ€è·å–ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä»config.jsonè¯»å–ï¼‰
        
        æ³¨æ„ï¼šå¯¹äºç‰¹å¾å‘é‡èšç±»ï¼Œåº”è¯¥ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼ˆç±»ä¼¼äººè„¸èšç±»ï¼‰
        äººè„¸èšç±»ä½¿ç”¨0.7ï¼Œè¿™é‡Œä¹Ÿä½¿ç”¨0.7ï¼Œç¡®ä¿åªæœ‰çœŸæ­£ç›¸ä¼¼çš„ç…§ç‰‡æ‰ä¼šè¢«åˆ†åˆ°åŒä¸€èšç±»
        """
        # ä¼˜å…ˆä½¿ç”¨image_featuresé…ç½®ä¸­çš„é˜ˆå€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨0.7ï¼ˆå‚è€ƒäººè„¸èšç±»ï¼‰
        if hasattr(settings, 'image_features') and hasattr(settings.image_features, 'similarity_threshold'):
            return settings.image_features.similarity_threshold
        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨0.7ï¼ˆæ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼Œç±»ä¼¼äººè„¸èšç±»ï¼‰
        return 0.7
    
    @property
    def large_cluster_threshold(self) -> int:
        """
        å¤§èšç±»é˜ˆå€¼ï¼šè¶…è¿‡æ­¤æ•°é‡çš„èšç±»å°†è¢«äºŒæ¬¡ç»†åˆ†
        """
        return 50  # é»˜è®¤50å¼ ç…§ç‰‡
    
    @property
    def refined_similarity_threshold(self) -> float:
        """
        äºŒæ¬¡ç»†åˆ†çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        
        è®¡ç®—æ–¹å¼ï¼šå°†ä½™å¼¦è·ç¦»é˜ˆå€¼å‡åŠ
        - åŸå§‹ç›¸ä¼¼åº¦é˜ˆå€¼å¯¹åº”çš„ä½™å¼¦è·ç¦» = 1 - similarity_threshold
        - ç»†åŒ–åçš„ä½™å¼¦è·ç¦» = (1 - similarity_threshold) / 2
        - ç»†åŒ–åçš„ç›¸ä¼¼åº¦é˜ˆå€¼ = 1 - (1 - similarity_threshold) / 2
        
        ä¾‹å¦‚ï¼šåŸå§‹é˜ˆå€¼0.78 â†’ è·ç¦»0.22 â†’ ç»†åŒ–è·ç¦»0.11 â†’ ç»†åŒ–é˜ˆå€¼0.89
        """
        # è®¡ç®—åŸå§‹ä½™å¼¦è·ç¦»
        original_distance = 1 - self.similarity_threshold
        # å°†è·ç¦»å‡åŠ
        refined_distance = original_distance / 2
        # è½¬æ¢å›ç›¸ä¼¼åº¦é˜ˆå€¼
        refined_threshold = 1 - refined_distance
        # é™åˆ¶æœ€å¤§å€¼ï¼Œé¿å…é˜ˆå€¼è¿‡é«˜å¯¼è‡´æ— æ³•èšç±»
        return min(0.95, refined_threshold)
    
    async def process_cluster_task(self, task_id: str) -> bool:
        """
        å¤„ç†èšç±»ä»»åŠ¡ï¼ˆå‚è€ƒäººè„¸è¯†åˆ«ä»»åŠ¡çš„process_face_recognition_taskï¼‰
        åœ¨åå°ä»»åŠ¡å†…éƒ¨åˆ›å»ºæ•°æ®åº“ä¼šè¯ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        
        :param task_id: ä»»åŠ¡ID
        :return: æ˜¯å¦èšç±»æˆåŠŸ
        """
        # ğŸ”¥ åœ¨åå°ä»»åŠ¡å†…éƒ¨åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯ï¼ˆå‚è€ƒäººè„¸è¯†åˆ«ä»»åŠ¡ï¼‰
        db = next(get_db())
        
        try:
            # è°ƒç”¨å®é™…çš„èšç±»æ–¹æ³•
            result = await self.cluster_similar_photos(db, task_id)
            return result
        except Exception as e:
            # ğŸ”¥ æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œé˜²æ­¢è¿›ç¨‹é€€å‡º
            logger.error(f"èšç±»ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            import traceback
            traceback.print_exc()
            if task_id:
                cluster_task_status[task_id] = {
                    "status": "failed",
                    "message": f"èšç±»ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}",
                    "end_time": datetime.now().isoformat()
                }
            return False
        finally:
            # ç¡®ä¿å…³é—­æ•°æ®åº“ä¼šè¯
            try:
                db.close()
            except Exception as e:
                logger.error(f"å…³é—­æ•°æ®åº“ä¼šè¯å¤±è´¥: {str(e)}")
    
    async def cluster_similar_photos(self, db: Session, task_id: Optional[str] = None) -> bool:
        """
        å…¨é‡ç›¸ä¼¼ç…§ç‰‡èšç±»åˆ†æ
        
        :param db: æ•°æ®åº“ä¼šè¯
        :param task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
        :return: æ˜¯å¦èšç±»æˆåŠŸ
        """
        _lazy_import_dependencies()
        
        if np is None or DBSCAN is None:
            logger.error("èšç±»ä¾èµ–åº“æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œèšç±»")
            if task_id:
                cluster_task_status[task_id] = {
                    "status": "failed",
                    "message": "èšç±»ä¾èµ–åº“æœªåŠ è½½"
                }
            return False
        
        # å¦‚æœæ²¡æœ‰æä¾›task_idï¼Œç”Ÿæˆä¸€ä¸ª
        if not task_id:
            task_id = f"cluster_{int(datetime.now().timestamp())}"
        
        try:
            # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
            cluster_task_status[task_id] = {
                "status": "processing",
                "message": "èšç±»åˆ†æè¿›è¡Œä¸­",
                "start_time": datetime.now().isoformat(),
                "current_stage": "initial_clustering",
                "progress_percentage": 0.0,
                "cluster_count": 0,
                "refined_count": 0,
                "total_photos": 0
            }
            
            logger.info(f"å¼€å§‹ç›¸ä¼¼ç…§ç‰‡èšç±»åˆ†æï¼ˆä»»åŠ¡ID: {task_id}ï¼‰...")
            
            # 1. åˆ é™¤æ‰€æœ‰æ—§èšç±»ï¼ˆåªåˆ é™¤æœ‰cluster_idçš„èšç±»ï¼Œä¿ç•™æ—§æ•°æ®ï¼‰
            logger.info("æ¸…ç†æ—§èšç±»æ•°æ®...")
            self._update_task_status(task_id, {
                "message": "æ¸…ç†æ—§èšç±»æ•°æ®...",
                "progress_percentage": 5.0
            })
            
            # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread() åŒ…è£…åŒæ­¥æ•°æ®åº“æ“ä½œï¼ˆå‚è€ƒäººè„¸è¯†åˆ«ä»»åŠ¡ï¼‰
            def cleanup_old_clusters():
                db.query(DuplicateGroupPhoto).filter(
                    DuplicateGroupPhoto.cluster_id.isnot(None)
                ).delete()
                db.query(DuplicateGroup).filter(
                    DuplicateGroup.cluster_id.isnot(None)
                ).delete()
                db.commit()
            
            await asyncio.to_thread(cleanup_old_clusters)
            
            # 2. è·å–æ‰€æœ‰å·²æå–ç‰¹å¾çš„ç…§ç‰‡ï¼ˆæŒ‰IDæ’åºï¼Œç¡®ä¿é¡ºåºå›ºå®šï¼‰
            def query_photos():
                return db.query(Photo).filter(
                    Photo.image_features_extracted == True,
                    Photo.image_features.isnot(None),
                    Photo.image_features != ''
                ).order_by(Photo.id).all()
            
            photos = await asyncio.to_thread(query_photos)
            
            logger.info(f"å¾…èšç±»ç…§ç‰‡æ•°é‡: {len(photos)}")
            self._update_task_status(task_id, {
                "message": f"å‡†å¤‡èšç±» {len(photos)} å¼ ç…§ç‰‡...",
                "progress_percentage": 10.0,
                "total_photos": len(photos)
            })
            
            if len(photos) < 2:
                logger.info("ç…§ç‰‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
                cluster_task_status[task_id] = {
                    "status": "completed",
                    "message": "ç…§ç‰‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»",
                    "progress_percentage": 100.0,
                    "end_time": datetime.now().isoformat()
                }
                return True
            
            # ğŸ”¥ æ£€æŸ¥ç…§ç‰‡æ•°é‡ï¼Œå¦‚æœå¤ªå¤šåˆ™è­¦å‘Š
            if len(photos) > 20000:
                logger.warning(f"ç…§ç‰‡æ•°é‡è¾ƒå¤š ({len(photos)} å¼ )ï¼Œèšç±»å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’Œå¤§é‡å†…å­˜")
                self._update_task_status(task_id, {
                    "message": f"ç…§ç‰‡æ•°é‡è¾ƒå¤š ({len(photos)} å¼ )ï¼Œæ­£åœ¨å‡†å¤‡èšç±»ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...",
                    "progress_percentage": 10.0
                })
            
            # 3. åˆ›å»ºæ–°èšç±»
            cluster_count = await self._create_new_clusters(photos, db, task_id)
            
            # æ›´æ–°è¿›åº¦ï¼šåˆå§‹èšç±»å®Œæˆ
            self._update_task_status(task_id, {
                "message": f"åˆå§‹èšç±»å®Œæˆï¼Œåˆ›å»ºäº† {cluster_count} ä¸ªèšç±»ï¼Œå¼€å§‹ç»†åˆ†å¤§èšç±»...",
                "progress_percentage": 70.0,
                "cluster_count": cluster_count
            })
            
            # 4. å¯¹å¤§èšç±»è¿›è¡ŒäºŒæ¬¡ç»†åˆ†
            refined_count = 0
            if cluster_count > 0:
                refined_count = await self._refine_large_clusters(db, task_id)
                if refined_count > 0:
                    logger.info(f"âœ… æˆåŠŸç»†åˆ† {refined_count} ä¸ªå¤§èšç±»")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            cluster_task_status[task_id] = {
                "status": "completed",
                "message": f"èšç±»å®Œæˆï¼Œå…±åˆ›å»º {cluster_count} ä¸ªèšç±»ï¼Œç»†åˆ† {refined_count} ä¸ªå¤§èšç±»",
                "cluster_count": cluster_count,
                "refined_count": refined_count,
                "progress_percentage": 100.0,
                "end_time": datetime.now().isoformat()
            }
            
            logger.info("ç›¸ä¼¼ç…§ç‰‡èšç±»å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼ç…§ç‰‡èšç±»å¤±è´¥: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            db.rollback()
            if task_id:
                cluster_task_status[task_id] = {
                    "status": "failed",
                    "message": f"èšç±»å¤±è´¥: {str(e)}",
                    "end_time": datetime.now().isoformat()
                }
            return False
    
    def _update_task_status(self, task_id: str, updates: dict):
        """
        æ›´æ–°ä»»åŠ¡çŠ¶æ€
        
        :param task_id: ä»»åŠ¡ID
        :param updates: è¦æ›´æ–°çš„å­—æ®µå­—å…¸
        """
        if task_id and task_id in cluster_task_status:
            cluster_task_status[task_id].update(updates)
    
    async def _create_new_clusters(self, photos: List[Photo], db: Session, task_id: Optional[str] = None) -> int:
        """
        åˆ›å»ºæ–°èšç±»ï¼ˆä½¿ç”¨DBSCANï¼‰
        
        :param photos: ç…§ç‰‡åˆ—è¡¨
        :param db: æ•°æ®åº“ä¼šè¯
        :return: åˆ›å»ºçš„èšç±»æ•°é‡
        """
        # ç¡®ä¿ä¾èµ–åº“å·²åŠ è½½
        _lazy_import_dependencies()
        
        if np is None or DBSCAN is None:
            logger.error("èšç±»ä¾èµ–åº“æœªåŠ è½½ï¼Œæ— æ³•åˆ›å»ºèšç±»")
            return 0
        
        if len(photos) < 1:
            logger.info("ç…§ç‰‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
            return 0
        
        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread() åŒ…è£…ç‰¹å¾å‘é‡æå–ï¼ˆé¿å…é˜»å¡ï¼‰
        def extract_features():
            features = []
            photo_ids = []
            for photo in photos:
                if photo.image_features:
                    try:
                        # è§£æJSONæ ¼å¼çš„ç‰¹å¾å‘é‡
                        if isinstance(photo.image_features, str):
                            feature_vector = json.loads(photo.image_features)
                        else:
                            feature_vector = photo.image_features
                        
                        if isinstance(feature_vector, list) and len(feature_vector) > 0:
                            features.append(feature_vector)
                            photo_ids.append(photo.id)
                    except Exception as e:
                        logger.warning(f"è§£æç…§ç‰‡ {photo.id} çš„ç‰¹å¾å‘é‡å¤±è´¥: {str(e)}")
                        continue
            return features, photo_ids
        
        features, photo_ids = await asyncio.to_thread(extract_features)
        
        if len(features) < 1:
            logger.info("æœ‰æ•ˆç‰¹å¾å‘é‡ä¸è¶³ï¼Œè·³è¿‡èšç±»")
            return 0
        
        # ğŸ”¥ ä½¿ç”¨ run_in_executor() åŒ…è£… NumPy å’Œ DBSCAN æ“ä½œï¼ˆCPUå¯†é›†å‹ï¼‰
        def perform_clustering():
            try:
                features_array = np.array(features)
                logger.info(f"æå–äº† {len(features_array)} ä¸ªæœ‰æ•ˆç‰¹å¾å‘é‡ï¼Œç»´åº¦: {features_array.shape}")
                
                # ğŸ”¥ æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼ˆä¼°ç®—ï¼‰
                # 34914 * 2048 * 4 bytes â‰ˆ 286MB (ä»…ç‰¹å¾å‘é‡)
                # DBSCANè·ç¦»çŸ©é˜µå¯èƒ½éœ€è¦æ›´å¤šå†…å­˜
                estimated_memory_mb = (len(features_array) * features_array.shape[1] * 4) / (1024 * 1024)
                logger.info(f"ä¼°ç®—å†…å­˜ä½¿ç”¨: {estimated_memory_mb:.1f}MB (ä»…ç‰¹å¾å‘é‡)")
                
                # ä½¿ç”¨DBSCANè¿›è¡Œèšç±»
                eps = 1 - self.similarity_threshold
                min_samples = 2
                
                logger.info(f"DBSCANå‚æ•°: eps={eps:.3f}, min_samples={min_samples}, metric='cosine'")
                logger.info(f"å¼€å§‹DBSCANèšç±»è®¡ç®—ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
                
                clustering = DBSCAN(
                    eps=eps,
                    min_samples=min_samples,
                    metric='cosine'
                )
                cluster_labels = clustering.fit_predict(features_array)
                
                logger.info(f"DBSCANèšç±»è®¡ç®—å®Œæˆ")
                return features_array, cluster_labels
            except MemoryError as e:
                logger.error(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•å®Œæˆèšç±»: {str(e)}")
                raise Exception(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç† {len(features)} å¼ ç…§ç‰‡ã€‚å»ºè®®åˆ†æ‰¹å¤„ç†æˆ–å‡å°‘ç…§ç‰‡æ•°é‡ã€‚")
            except Exception as e:
                logger.error(f"DBSCANèšç±»è®¡ç®—å¤±è´¥: {str(e)}", exc_info=True)
                raise
        
        # ğŸ”¥ åœ¨äº‹ä»¶å¾ªç¯çš„çº¿ç¨‹æ± ä¸­æ‰§è¡ŒCPUå¯†é›†å‹æ“ä½œï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†
        try:
            features_array, cluster_labels = await asyncio.get_event_loop().run_in_executor(
                None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                perform_clustering
            )
        except Exception as e:
            logger.error(f"æ‰§è¡Œèšç±»è®¡ç®—å¤±è´¥: {str(e)}", exc_info=True)
            if task_id:
                self._update_task_status(task_id, {
                    "status": "failed",
                    "message": f"èšç±»è®¡ç®—å¤±è´¥: {str(e)}",
                    "end_time": datetime.now().isoformat()
                })
            raise
        
        # æ›´æ–°è¿›åº¦ï¼šç‰¹å¾æå–å®Œæˆ
        if task_id:
            self._update_task_status(task_id, {
                "message": f"å·²æå– {len(features_array)} ä¸ªç‰¹å¾å‘é‡ï¼Œå¼€å§‹èšç±»...",
                "progress_percentage": 30.0
            })
        
        # å¤„ç†èšç±»ç»“æœ
        unique_labels = set(cluster_labels)
        if -1 in unique_labels:
            unique_labels.remove(-1)  # ç§»é™¤å™ªå£°ç‚¹
        
        logger.info(f"æ£€æµ‹åˆ° {len(unique_labels)} ä¸ªèšç±»ï¼Œå™ªå£°ç‚¹: {sum(cluster_labels == -1)}")
        
        # æ›´æ–°è¿›åº¦ï¼šèšç±»è®¡ç®—å®Œæˆ
        if task_id:
            self._update_task_status(task_id, {
                "message": f"æ£€æµ‹åˆ° {len(unique_labels)} ä¸ªèšç±»ï¼Œæ­£åœ¨åˆ›å»ºèšç±»è®°å½•...",
                "progress_percentage": 50.0
            })
        
        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread() åŒ…è£…æ•°æ®åº“æ“ä½œï¼ˆåˆ›å»ºèšç±»è®°å½•ï¼‰
        def create_cluster_records():
            try:
                clusters_info = []  # [(cluster_id, cluster_photo_ids, size, avg_similarity)]
                total_clusters = len(unique_labels)
                logger.info(f"å¼€å§‹åˆ›å»º {total_clusters} ä¸ªèšç±»è®°å½•...")
                
                for idx, cluster_label in enumerate(unique_labels):
                    # ğŸ”¥ æ·»åŠ è¿›åº¦æ—¥å¿—
                    if (idx + 1) % 50 == 0 or idx == 0:
                        logger.info(f"æ­£åœ¨åˆ›å»ºèšç±»è®°å½•: {idx + 1}/{total_clusters}")
                    
                    cluster_photo_ids = [photo_ids[i] for i, label in enumerate(cluster_labels) if label == cluster_label]
                    
                    if len(cluster_photo_ids) < 1:
                        continue
                    
                    # ğŸ”¥ ç”Ÿæˆå”¯ä¸€çš„cluster_idï¼ˆä½¿ç”¨UUIDç¡®ä¿å”¯ä¸€æ€§ï¼‰
                    uuid_short = uuid.uuid4().hex[:8]  # ä½¿ç”¨8ä½UUIDç¡®ä¿å”¯ä¸€æ€§
                    cluster_id = f"cluster_{cluster_label}_{uuid_short}"
                    
                    # ğŸ”¥ å¯¹äºå¤§èšç±»ï¼Œä½¿ç”¨ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
                    cluster_features = [features_array[i] for i, label in enumerate(cluster_labels) if label == cluster_label]
                    
                    # å¦‚æœèšç±»å¤ªå¤§ï¼ˆ>500å¼ ï¼‰ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
                    if len(cluster_features) > 500:
                        logger.warning(f"èšç±» {cluster_label} åŒ…å« {len(cluster_features)} å¼ ç…§ç‰‡ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦")
                        # é‡‡æ ·å‰100å¼ å’Œå100å¼ ç…§ç‰‡è®¡ç®—ç›¸ä¼¼åº¦
                        sample_size = min(100, len(cluster_features) // 2)
                        sample_features = cluster_features[:sample_size] + cluster_features[-sample_size:]
                        avg_similarity = self._calculate_cluster_avg_similarity(sample_features)
                    else:
                        avg_similarity = self._calculate_cluster_avg_similarity(cluster_features)
                    
                    # è®¡ç®—èšç±»è´¨é‡
                    cluster_quality = self._calculate_cluster_quality(len(cluster_photo_ids), avg_similarity)
                    
                    # åˆ›å»ºèšç±»è®°å½•
                    representative_photo_id = cluster_photo_ids[0] if cluster_photo_ids else None
                    
                    cluster = DuplicateGroup(
                        cluster_id=cluster_id,
                        representative_photo_id=representative_photo_id,
                        photo_count=len(cluster_photo_ids),
                        avg_similarity=avg_similarity,
                        confidence_score=avg_similarity,
                        cluster_quality=cluster_quality,
                        similarity_threshold=self.similarity_threshold
                    )
                    db.add(cluster)
                    db.flush()  # è·å–group_id
                    
                    # æ·»åŠ èšç±»æˆå‘˜
                    for photo_id in cluster_photo_ids:
                        # è®¡ç®—è¯¥ç…§ç‰‡ä¸èšç±»ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
                        photo_idx = photo_ids.index(photo_id)
                        photo_feature = features_array[photo_idx]
                        
                        # ğŸ”¥ å¯¹äºå¤§èšç±»ï¼Œä½¿ç”¨é‡‡æ ·ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦
                        if len(cluster_features) > 500:
                            similarity_score = self._calculate_photo_cluster_similarity(photo_feature, sample_features)
                        else:
                            similarity_score = self._calculate_photo_cluster_similarity(photo_feature, cluster_features)
                        
                        member = DuplicateGroupPhoto(
                            cluster_id=cluster_id,
                            group_id=cluster.id,
                            photo_id=photo_id,
                            similarity_score=similarity_score
                        )
                        db.add(member)
                    
                    clusters_info.append((cluster_id, cluster_photo_ids, len(cluster_photo_ids), avg_similarity))
                
                logger.info(f"å¼€å§‹æäº¤æ•°æ®åº“äº‹åŠ¡ï¼ˆåˆ›å»º {len(clusters_info)} ä¸ªèšç±»ï¼‰...")
                db.commit()
                logger.info(f"æ•°æ®åº“äº‹åŠ¡æäº¤å®Œæˆ")
                return clusters_info
            except MemoryError as e:
                logger.error(f"åˆ›å»ºèšç±»è®°å½•æ—¶å†…å­˜ä¸è¶³: {str(e)}", exc_info=True)
                db.rollback()
                raise Exception(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºèšç±»è®°å½•ã€‚å»ºè®®å‡å°‘ç…§ç‰‡æ•°é‡æˆ–åˆ†æ‰¹å¤„ç†ã€‚")
            except Exception as e:
                logger.error(f"åˆ›å»ºèšç±»è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
                db.rollback()
                raise
        
        clusters_info = await asyncio.to_thread(create_cluster_records)
        
        logger.info(f"âœ… æˆåŠŸåˆ›å»º {len(clusters_info)} ä¸ªèšç±»")
        
        # æ›´æ–°è¿›åº¦ï¼šèšç±»åˆ›å»ºå®Œæˆ
        if task_id:
            self._update_task_status(task_id, {
                "message": f"æˆåŠŸåˆ›å»º {len(clusters_info)} ä¸ªèšç±»",
                "progress_percentage": 65.0
            })
        
        # ç»Ÿè®¡ä¿¡æ¯
        if clusters_info:
            total_photos = sum(size for _, _, size, _ in clusters_info)
            avg_cluster_size = total_photos / len(clusters_info)
            logger.info(f"èšç±»ç»Ÿè®¡: æ€»ç…§ç‰‡æ•°={total_photos}, å¹³å‡èšç±»å¤§å°={avg_cluster_size:.1f}")
        
        return len(clusters_info)
    
    async def _refine_large_clusters(self, db: Session, task_id: Optional[str] = None) -> int:
        """
        å¯¹å¤§èšç±»è¿›è¡Œé€’å½’äºŒæ¬¡ç»†åˆ†ï¼Œç›´åˆ°æ‰€æœ‰èšç±»éƒ½å°äºç­‰äºé˜ˆå€¼
        
        :param db: æ•°æ®åº“ä¼šè¯
        :return: ç»†åˆ†çš„èšç±»æ•°é‡
        """
        _lazy_import_dependencies()
        
        if np is None or DBSCAN is None:
            logger.error("èšç±»ä¾èµ–åº“æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡ŒäºŒæ¬¡ç»†åˆ†")
            return 0
        
        try:
            refined_count = 0
            max_iterations = 15  # å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œé€‚åº”æ›´å¹³ç¼“çš„è¿­ä»£è¶‹åŠ¿ï¼ˆ5/6ç³»æ•°ï¼‰
            iteration = 0
            
            # ğŸ”¥ åœ¨å†…å­˜ä¸­ç»´æŠ¤å¾…ç»†åˆ†çš„ç…§ç‰‡IDåˆ—è¡¨ï¼ˆå¤§äºé˜ˆå€¼çš„èšç±»ï¼‰
            # æ ¼å¼ï¼š{iteration: [photo_ids_list1, photo_ids_list2, ...]}
            pending_clusters_in_memory = []  # å­˜å‚¨å¾…ç»†åˆ†çš„ç…§ç‰‡IDåˆ—è¡¨
            
            # è®¡ç®—åˆå§‹ä½™å¼¦è·ç¦»
            base_distance = 1 - self.similarity_threshold
            min_distance = 0.01  # æœ€å°è·ç¦»é™åˆ¶ï¼ˆå¯¹åº”æœ€å¤§ç›¸ä¼¼åº¦é˜ˆå€¼0.99ï¼‰
            
            while iteration < max_iterations:
                # è®¡ç®—å½“å‰è¿­ä»£çš„é˜ˆå€¼
                # ğŸ”¥ ä½¿ç”¨ä¹˜ä»¥5/6çš„æ–¹å¼ï¼Œè®©é˜ˆå€¼å˜åŒ–æ›´å¹³ç¼“ï¼ˆæ”¶æ•›æ›´æ…¢ï¼Œå‡å°‘å™ªå£°ç‚¹ï¼‰
                # ç¬¬1æ¬¡è¿­ä»£ï¼šbase * (5/6)^1, ç¬¬2æ¬¡è¿­ä»£ï¼šbase * (5/6)^2, ç¬¬3æ¬¡è¿­ä»£ï¼šbase * (5/6)^3...
                current_distance = base_distance * ((5/6) ** (iteration + 1))
                
                # ç¡®ä¿è·ç¦»ä¸å°äºæœ€å°å€¼
                if current_distance < min_distance:
                    current_distance = min_distance
                
                # è½¬æ¢ä¸ºç›¸ä¼¼åº¦é˜ˆå€¼
                current_threshold = 1 - current_distance
                
                # ğŸ”¥ ä»æ•°æ®åº“å’Œå†…å­˜ä¸­è·å–å¾…ç»†åˆ†çš„èšç±»
                # 1. ä»æ•°æ®åº“æŸ¥è¯¢å¤§äºé˜ˆå€¼çš„èšç±»
                def query_large_clusters():
                    return db.query(DuplicateGroup).filter(
                        DuplicateGroup.cluster_id.isnot(None),
                        DuplicateGroup.photo_count > self.large_cluster_threshold
                    ).all()
                
                large_clusters = await asyncio.to_thread(query_large_clusters)
                
                # 2. ä»å†…å­˜ä¸­è·å–å¾…ç»†åˆ†çš„ç…§ç‰‡IDåˆ—è¡¨
                # å°†å†…å­˜ä¸­çš„ç…§ç‰‡IDåˆ—è¡¨è½¬æ¢ä¸ºç±»ä¼¼æ•°æ®åº“æŸ¥è¯¢çš„æ ¼å¼
                memory_clusters = []
                if pending_clusters_in_memory:
                    # ä¸ºå†…å­˜ä¸­çš„æ¯ä¸ªç…§ç‰‡IDåˆ—è¡¨åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„èšç±»å¯¹è±¡
                    for photo_ids in pending_clusters_in_memory:
                        if len(photo_ids) > self.large_cluster_threshold:
                            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¯¹è±¡æ¥æ¨¡æ‹Ÿ DuplicateGroup
                            class MemoryCluster:
                                def __init__(self, photo_ids):
                                    self.cluster_id = f"memory_{id(photo_ids)}"  # ä¸´æ—¶ID
                                    self.photo_ids = photo_ids
                                    self.photo_count = len(photo_ids)
                            memory_clusters.append(MemoryCluster(photo_ids))
                
                # æ¸…ç©ºå†…å­˜åˆ—è¡¨ï¼Œæœ¬æ¬¡è¿­ä»£ä¼šé‡æ–°å¡«å……
                pending_clusters_in_memory = []
                
                # åˆå¹¶æ•°æ®åº“å’Œå†…å­˜ä¸­çš„èšç±»
                all_large_clusters = list(large_clusters) + memory_clusters
                
                if not all_large_clusters:
                    logger.info(f"ç¬¬ {iteration + 1} æ¬¡è¿­ä»£ï¼šæ²¡æœ‰éœ€è¦ç»†åˆ†çš„å¤§èšç±»ï¼Œç»“æŸé€’å½’")
                    break
                
                logger.info(f"ç¬¬ {iteration + 1} æ¬¡è¿­ä»£ï¼šå‘ç° {len(all_large_clusters)} ä¸ªå¤§èšç±»éœ€è¦ç»†åˆ†"
                          f"ï¼ˆæ•°æ®åº“: {len(large_clusters)}, å†…å­˜: {len(memory_clusters)}ï¼‰ï¼Œ"
                          f"ä½¿ç”¨é˜ˆå€¼={current_threshold:.3f}ï¼ˆè·ç¦»={current_distance:.4f}ï¼‰")
                
                # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹é€’å½’ç»†åˆ†è¿­ä»£
                if task_id:
                    # è¿›åº¦è®¡ç®—ï¼š70% + (å½“å‰è¿­ä»£ / æœ€å¤§è¿­ä»£æ•°) * 30%
                    base_progress = 70.0
                    iteration_progress = min(iteration / max_iterations, 1.0) * 30.0
                    self._update_task_status(task_id, {
                        "current_stage": "refining",
                        "message": f"ç¬¬ {iteration + 1} æ¬¡è¿­ä»£ï¼šæ­£åœ¨ç»†åˆ† {len(large_clusters)} ä¸ªå¤§èšç±»...",
                        "progress_percentage": base_progress + iteration_progress * 0.3,  # å…ˆè®¾ç½®ä¸€ä¸ªåŸºç¡€è¿›åº¦
                        "refining_iteration": iteration + 1,
                        "refining_total_clusters": len(all_large_clusters),
                        "refining_processed_clusters": 0
                    })
                
                iteration_refined = 0
                
                for large_cluster in all_large_clusters:
                    try:
                        # ğŸ”¥ åˆ¤æ–­æ˜¯æ•°æ®åº“èšç±»è¿˜æ˜¯å†…å­˜èšç±»
                        is_memory_cluster = hasattr(large_cluster, 'photo_ids')
                        
                        if is_memory_cluster:
                            # å†…å­˜èšç±»ï¼šç›´æ¥ä½¿ç”¨ç…§ç‰‡IDåˆ—è¡¨
                            refined_photo_ids = large_cluster.photo_ids
                            logger.info(f"ç»†åˆ†å†…å­˜èšç±»ï¼ŒåŒ…å« {len(refined_photo_ids)} å¼ ç…§ç‰‡")
                            
                            # ä»æ•°æ®åº“æŸ¥è¯¢ç…§ç‰‡å¯¹è±¡ï¼ˆåˆ†æ‰¹æŸ¥è¯¢ï¼Œé¿å…SQLiteå‚æ•°é™åˆ¶ï¼‰
                            def get_cluster_photos_from_ids():
                                photos = []
                                batch_size = 999  # SQLite INå­å¥é™åˆ¶
                                for i in range(0, len(refined_photo_ids), batch_size):
                                    batch_ids = refined_photo_ids[i:i+batch_size]
                                    batch_photos = db.query(Photo).filter(
                                        Photo.id.in_(batch_ids),
                                        Photo.image_features.isnot(None),
                                        Photo.image_features != ''
                                    ).order_by(Photo.id).all()
                                    photos.extend(batch_photos)
                                return photos
                            
                            photos = await asyncio.to_thread(get_cluster_photos_from_ids)
                        else:
                            # æ•°æ®åº“èšç±»ï¼šä»æ•°æ®åº“æŸ¥è¯¢æˆå‘˜å’Œç…§ç‰‡
                            def get_cluster_members():
                                return db.query(DuplicateGroupPhoto).filter(
                                    DuplicateGroupPhoto.cluster_id == large_cluster.cluster_id
                                ).all()
                            
                            cluster_members = await asyncio.to_thread(get_cluster_members)
                            
                            if len(cluster_members) <= self.large_cluster_threshold:
                                continue
                            
                            logger.info(f"ç»†åˆ†èšç±» {large_cluster.cluster_id}ï¼ŒåŒ…å« {len(cluster_members)} å¼ ç…§ç‰‡")
                            
                            # ğŸ”¥ ä½¿ç”¨ JOIN æ–¹å¼æŸ¥è¯¢ï¼Œé¿å… IN å­å¥çš„ SQLite 999 å‚æ•°é™åˆ¶
                            def get_cluster_photos():
                                # ä½¿ç”¨ JOIN æŸ¥è¯¢ï¼Œä¸éœ€è¦ IN å­å¥ï¼Œé¿å… SQLite å‚æ•°é™åˆ¶
                                photos = db.query(Photo).join(
                                    DuplicateGroupPhoto, Photo.id == DuplicateGroupPhoto.photo_id
                                ).filter(
                                    DuplicateGroupPhoto.cluster_id == large_cluster.cluster_id,
                                    Photo.image_features.isnot(None),
                                    Photo.image_features != ''
                                ).order_by(Photo.id).all()
                                
                                return photos
                            
                            photos = await asyncio.to_thread(get_cluster_photos)
                            refined_photo_ids = [photo.id for photo in photos]
                        
                        if len(photos) < 2:
                            logger.warning(f"èšç±» {large_cluster.cluster_id} çš„æœ‰æ•ˆç…§ç‰‡ä¸è¶³ï¼Œè·³è¿‡ç»†åˆ†")
                            continue
                        
                        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread() åŒ…è£…ç‰¹å¾å‘é‡æå–
                        def extract_refined_features():
                            features = []
                            valid_photo_ids = []
                            for photo in photos:
                                if photo.image_features:
                                    try:
                                        if isinstance(photo.image_features, str):
                                            feature_vector = json.loads(photo.image_features)
                                        else:
                                            feature_vector = photo.image_features
                                        
                                        if isinstance(feature_vector, list) and len(feature_vector) > 0:
                                            features.append(feature_vector)
                                            valid_photo_ids.append(photo.id)
                                    except Exception as e:
                                        logger.warning(f"è§£æç…§ç‰‡ {photo.id} çš„ç‰¹å¾å‘é‡å¤±è´¥: {str(e)}")
                                        continue
                            return features, valid_photo_ids
                        
                        features, valid_photo_ids = await asyncio.to_thread(extract_refined_features)
                        # æ›´æ–° refined_photo_ids ä¸ºæœ‰æ•ˆçš„ç…§ç‰‡ID
                        refined_photo_ids = valid_photo_ids
                        
                        if len(features) < 2:
                            logger.warning(f"èšç±» {large_cluster.cluster_id} çš„æœ‰æ•ˆç‰¹å¾å‘é‡ä¸è¶³ï¼Œè·³è¿‡ç»†åˆ†")
                            continue
                        
                        # ğŸ”¥ ä½¿ç”¨ run_in_executor() åŒ…è£… DBSCAN èšç±»ï¼ˆCPUå¯†é›†å‹ï¼‰
                        def perform_refined_clustering():
                            features_array = np.array(features)
                            
                            # ä½¿ç”¨å½“å‰è¿­ä»£çš„é˜ˆå€¼è¿›è¡Œèšç±»
                            refined_eps = current_distance
                            min_samples = 2
                            
                            logger.info(f"èšç±»å‚æ•°: eps={refined_eps:.4f}, min_samples={min_samples}, "
                                      f"ç›¸ä¼¼åº¦é˜ˆå€¼={current_threshold:.3f}")
                            
                            refined_clustering = DBSCAN(
                                eps=refined_eps,
                                min_samples=min_samples,
                                metric='cosine'
                            )
                            refined_labels = refined_clustering.fit_predict(features_array)
                            
                            return features_array, refined_labels
                        
                        features_array, refined_labels = await asyncio.get_event_loop().run_in_executor(
                            None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                            perform_refined_clustering
                        )
                        
                        # å¤„ç†ç»†åˆ†ç»“æœ
                        refined_unique_labels = set(refined_labels)
                        if -1 in refined_unique_labels:
                            refined_unique_labels.remove(-1)  # ç§»é™¤å™ªå£°ç‚¹
                        
                        if len(refined_unique_labels) <= 1:
                            cluster_id = getattr(large_cluster, 'cluster_id', 'å†…å­˜èšç±»')
                            logger.info(f"èšç±» {cluster_id} ç»†åˆ†åä»ä¸º1ä¸ªèšç±»ï¼Œè·³è¿‡")
                            continue
                        
                        logger.info(f"èšç±» {getattr(large_cluster, 'cluster_id', 'å†…å­˜èšç±»')} ç»†åˆ†ä¸º {len(refined_unique_labels)} ä¸ªå­èšç±»")
                        
                        # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread() åŒ…è£…æ•°æ®åº“æ“ä½œï¼ˆåˆ é™¤åŸèšç±»å¹¶åˆ›å»ºæ–°èšç±»ï¼‰
                        def create_refined_clusters(is_mem_cluster):
                            try:
                                # ğŸ”¥ åˆ›å»º photo_id -> index å­—å…¸æ˜ å°„ï¼Œé¿å…é‡å¤çš„ O(n) æŸ¥æ‰¾
                                photo_id_to_index = {photo_id: idx for idx, photo_id in enumerate(refined_photo_ids)}
                                
                                # ğŸ”¥ åªåˆ é™¤æ•°æ®åº“èšç±»ï¼Œå†…å­˜èšç±»ä¸éœ€è¦åˆ é™¤
                                if not is_mem_cluster:
                                    # åˆ é™¤åŸèšç±»åŠå…¶æˆå‘˜
                                    db.query(DuplicateGroupPhoto).filter(
                                        DuplicateGroupPhoto.cluster_id == large_cluster.cluster_id
                                    ).delete()
                                    db.delete(large_cluster)
                                    db.flush()
                                
                                # ğŸ”¥ æ‰¹é‡åˆ›å»ºæ‰€æœ‰èšç±»å’Œæˆå‘˜ï¼Œå‡å°‘flushæ¬¡æ•°
                                refined_clusters = []  # å­˜å‚¨æ‰€æœ‰èšç±»å¯¹è±¡
                                pending_clusters = []  # å­˜å‚¨å¾…ç»†åˆ†çš„ç…§ç‰‡IDåˆ—è¡¨
                                
                                # ç¬¬ä¸€éï¼šåªåˆ›å»ºæœ€ç»ˆç¡®å®šçš„èšç±»ï¼ˆ<=é˜ˆå€¼ï¼‰ï¼Œå¤§äºé˜ˆå€¼çš„æ”¾åœ¨å†…å­˜ä¸­
                                for refined_label in refined_unique_labels:
                                    refined_photo_ids_subset = [refined_photo_ids[i] 
                                                                for i, label in enumerate(refined_labels) 
                                                                if label == refined_label]
                                    
                                    if len(refined_photo_ids_subset) < 1:
                                        continue
                                    
                                    cluster_size = len(refined_photo_ids_subset)
                                    
                                    if cluster_size > self.large_cluster_threshold:
                                        # ğŸ”¥ å¤§äºé˜ˆå€¼çš„èšç±»ï¼šæ”¾åœ¨å†…å­˜ä¸­ï¼Œä¸åˆ›å»ºæ•°æ®åº“è®°å½•
                                        pending_clusters.append(refined_photo_ids_subset)
                                        logger.debug(f"å­èšç±»åŒ…å« {cluster_size} å¼ ç…§ç‰‡ï¼ˆ>é˜ˆå€¼ï¼‰ï¼Œæ”¾å…¥å†…å­˜ï¼Œå°†åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ç»§ç»­ç»†åˆ†")
                                        continue
                                    
                                    # ğŸ”¥ åªåˆ›å»º <= é˜ˆå€¼çš„èšç±»ï¼ˆæœ€ç»ˆç¡®å®šçš„èšç±»ï¼‰
                                    uuid_short = uuid.uuid4().hex[:8]
                                    refined_cluster_id = f"cluster_refined_{iteration}_{refined_label}_{uuid_short}"
                                    
                                    # è®¡ç®—çœŸå®å¹³å‡ç›¸ä¼¼åº¦
                                    refined_cluster_features = [features_array[i] 
                                                                for i, label in enumerate(refined_labels) 
                                                                if label == refined_label]
                                    avg_similarity = self._calculate_cluster_avg_similarity(refined_cluster_features)
                                    
                                    # è®¡ç®—èšç±»è´¨é‡
                                    cluster_quality = self._calculate_cluster_quality(cluster_size, avg_similarity)
                                    
                                    # åˆ›å»ºæ–°çš„èšç±»è®°å½•
                                    refined_cluster = DuplicateGroup(
                                        cluster_id=refined_cluster_id,
                                        representative_photo_id=refined_photo_ids_subset[0],
                                        photo_count=cluster_size,
                                        avg_similarity=avg_similarity,
                                        confidence_score=avg_similarity,
                                        cluster_quality=cluster_quality,
                                        similarity_threshold=current_threshold
                                    )
                                    db.add(refined_cluster)
                                    refined_clusters.append((refined_cluster, refined_cluster_id, refined_photo_ids_subset, refined_cluster_features))
                                
                                # ğŸ”¥ æ‰¹é‡flushè·å–æ‰€æœ‰group_idï¼ˆåªflushä¸€æ¬¡ï¼‰
                                logger.info(f"æ‰¹é‡flush {len(refined_clusters)} ä¸ªèšç±»ä»¥è·å–group_id...")
                                db.flush()
                                
                                # ç¬¬äºŒéï¼šåˆ›å»ºæ‰€æœ‰æˆå‘˜å¯¹è±¡
                                for refined_cluster, refined_cluster_id, refined_photo_ids_subset, refined_cluster_features in refined_clusters:
                                    # ğŸ”¥ å¯¹äºå¤§èšç±»ï¼Œä½¿ç”¨é‡‡æ ·ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä¸ç¬¬ä¸€æ¬¡åˆ›å»ºèšç±»ä¿æŒä¸€è‡´ï¼‰
                                    if len(refined_cluster_features) > 500:
                                        # é‡‡æ ·å‰100å¼ å’Œå100å¼ ç…§ç‰‡çš„ç‰¹å¾
                                        sample_size = min(100, len(refined_cluster_features) // 2)
                                        sample_features = refined_cluster_features[:sample_size] + refined_cluster_features[-sample_size:]
                                    else:
                                        sample_features = refined_cluster_features
                                    
                                    for photo_id in refined_photo_ids_subset:
                                        # ğŸ”¥ ä½¿ç”¨å­—å…¸æ˜ å°„è¿›è¡Œ O(1) æŸ¥æ‰¾ï¼Œæ›¿ä»£ O(n) çš„ index() æ“ä½œ
                                        photo_idx = photo_id_to_index[photo_id]
                                        photo_feature = features_array[photo_idx]
                                        # ğŸ”¥ ä½¿ç”¨é‡‡æ ·ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå¦‚æœèšç±»å¤ªå¤§ï¼‰
                                        similarity_score = self._calculate_photo_cluster_similarity(
                                            photo_feature, sample_features
                                        )
                                        
                                        member = DuplicateGroupPhoto(
                                            cluster_id=refined_cluster_id,
                                            group_id=refined_cluster.id,
                                            photo_id=photo_id,
                                            similarity_score=similarity_score
                                        )
                                        db.add(member)
                                
                                logger.info(f"å·²åˆ›å»º {len(refined_clusters)} ä¸ªæœ€ç»ˆèšç±»ï¼Œ"
                                          f"{len(pending_clusters)} ä¸ªå¤§èšç±»æ”¾å…¥å†…å­˜å¾…ä¸‹æ¬¡è¿­ä»£")
                                return len(refined_clusters), pending_clusters
                            except Exception as e:
                                logger.error(f"åˆ›å»ºç»†åˆ†èšç±»è®°å½•å¤±è´¥: {str(e)}", exc_info=True)
                                db.rollback()
                                raise
                        
                        iteration_refined_local, new_pending_clusters = await asyncio.to_thread(create_refined_clusters, is_memory_cluster)
                        iteration_refined += iteration_refined_local
                        # ğŸ”¥ å°†æ–°çš„å¤§èšç±»æ·»åŠ åˆ°å†…å­˜åˆ—è¡¨
                        pending_clusters_in_memory.extend(new_pending_clusters)
                        
                        cluster_info = f"å†…å­˜èšç±»" if is_memory_cluster else f"èšç±» {large_cluster.cluster_id}"
                        logger.info(f"âœ… {cluster_info} ç»†åˆ†å®Œæˆï¼Œ"
                                  f"åŸ {len(refined_photo_ids)} å¼ ç…§ç‰‡åˆ†ä¸º {len(refined_unique_labels)} ä¸ªå­èšç±»")
                        
                        # ğŸ”¥ æ˜¾å¼é‡Šæ”¾å¤§å¯¹è±¡ï¼Œé‡Šæ”¾å†…å­˜ï¼ˆå¤„ç†å®Œä¸€ä¸ªå¤§èšç±»åï¼‰
                        del features_array
                        del refined_photo_ids
                        del refined_labels
                        del refined_unique_labels
                        
                        # æ›´æ–°è¿›åº¦ï¼šæ¯ä¸ªå¤§èšç±»å¤„ç†å®Œæˆ
                        if task_id:
                            # è®¡ç®—å½“å‰è¿­ä»£çš„è¿›åº¦
                            base_progress = 70.0
                            iteration_progress_range = 30.0  # é€’å½’ç»†åˆ†é˜¶æ®µå æ€»è¿›åº¦çš„30%
                            # å½“å‰è¿­ä»£çš„è¿›åº¦ = (å·²å¤„ç†èšç±»æ•° / æ€»èšç±»æ•°) * å½“å‰è¿­ä»£çš„è¿›åº¦èŒƒå›´
                            current_iteration_progress = (iteration_refined / len(all_large_clusters)) * (iteration_progress_range / max_iterations) if len(all_large_clusters) > 0 else 0
                            # æ€»è¿›åº¦ = 70% + ä¹‹å‰è¿­ä»£çš„è¿›åº¦ + å½“å‰è¿­ä»£çš„è¿›åº¦
                            total_progress = base_progress + (iteration / max_iterations) * iteration_progress_range + current_iteration_progress
                            
                            self._update_task_status(task_id, {
                                "message": f"ç¬¬ {iteration + 1} æ¬¡è¿­ä»£ï¼šå·²å¤„ç† {iteration_refined}/{len(all_large_clusters)} ä¸ªå¤§èšç±»...",
                                "progress_percentage": min(total_progress, 99.0),  # ä¸è¶…è¿‡99%ï¼Œç•™1%ç»™æœ€ç»ˆå®Œæˆ
                                "refining_processed_clusters": iteration_refined
                            })
                        
                    except Exception as e:
                        cluster_id = getattr(large_cluster, 'cluster_id', 'å†…å­˜èšç±»')
                        logger.error(f"ç»†åˆ†èšç±» {cluster_id} å¤±è´¥: {str(e)}")
                        continue
                
                refined_count += iteration_refined
                
                # å¦‚æœæ²¡æœ‰æˆåŠŸç»†åˆ†ä»»ä½•èšç±»ï¼Œé€€å‡ºå¾ªç¯
                if iteration_refined == 0:
                    logger.info(f"ç¬¬ {iteration + 1} æ¬¡è¿­ä»£ï¼šæ²¡æœ‰æˆåŠŸç»†åˆ†ä»»ä½•èšç±»ï¼Œç»“æŸé€’å½’")
                    break
                
                # ğŸ”¥ ä½¿ç”¨ asyncio.to_thread() åŒ…è£…æ•°æ®åº“æäº¤
                def commit_changes():
                    db.commit()
                
                await asyncio.to_thread(commit_changes)
                
                # ğŸ”¥ è®©å‡ºæ§åˆ¶æƒï¼Œå…è®¸å…¶ä»–ä»»åŠ¡æ‰§è¡Œï¼ˆå‚è€ƒäººè„¸è¯†åˆ«ä»»åŠ¡ï¼‰
                await asyncio.sleep(0.01)
                
                iteration += 1
            
            if iteration >= max_iterations:
                logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œåœæ­¢é€’å½’ç»†åˆ†")
            
            logger.info(f"é€’å½’ç»†åˆ†å®Œæˆï¼Œå…±è¿›è¡Œ {iteration} æ¬¡è¿­ä»£ï¼Œç»†åˆ†äº† {refined_count} ä¸ªèšç±»")
            return refined_count
            
        except Exception as e:
            logger.error(f"å¤§èšç±»é€’å½’ç»†åˆ†å¤±è´¥: {str(e)}")
            db.rollback()
            return 0
    
    def _calculate_cluster_avg_similarity(self, cluster_features: List) -> float:
        """
        è®¡ç®—èšç±»å†…å¹³å‡ç›¸ä¼¼åº¦
        
        :param cluster_features: èšç±»å†…çš„ç‰¹å¾å‘é‡åˆ—è¡¨
        :return: å¹³å‡ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        # ç¡®ä¿ä¾èµ–åº“å·²åŠ è½½
        _lazy_import_dependencies()
        
        if np is None:
            logger.warning("numpyæœªåŠ è½½ï¼Œæ— æ³•è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦")
            return 0.8  # è¿”å›é»˜è®¤å€¼
        
        if len(cluster_features) < 2:
            return 1.0
        
        try:
            features_array = np.array(cluster_features)
            
            # ğŸ”¥ å¯¹äºå¤§èšç±»ï¼ˆ>500ï¼‰ï¼Œä½¿ç”¨é‡‡æ ·æ–¹æ³•é¿å…å†…å­˜æº¢å‡º
            if len(features_array) > 500:
                # é‡‡æ ·è®¡ç®—ï¼šéšæœºé€‰æ‹©æœ€å¤š200ä¸ªæ ·æœ¬å¯¹
                sample_size = min(200, len(features_array))
                indices = np.random.choice(len(features_array), sample_size, replace=False)
                sample_features = features_array[indices]
                
                from sklearn.metrics.pairwise import cosine_similarity
                similarity_matrix = cosine_similarity(sample_features)
                
                # å–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
                upper_triangle = similarity_matrix[np.triu_indices(len(similarity_matrix), k=1)]
                
                # è¿”å›å¹³å‡å€¼
                return float(np.mean(upper_triangle))
            
            # å¯¹äºå°èšç±»ï¼Œä½¿ç”¨å®Œæ•´è®¡ç®—
            from sklearn.metrics.pairwise import cosine_similarity
            similarity_matrix = cosine_similarity(features_array)
            
            # å–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
            upper_triangle = similarity_matrix[np.triu_indices(len(similarity_matrix), k=1)]
            
            # è¿”å›å¹³å‡å€¼
            return float(np.mean(upper_triangle))
        except MemoryError as e:
            logger.error(f"è®¡ç®—èšç±»å¹³å‡ç›¸ä¼¼åº¦æ—¶å†…å­˜ä¸è¶³: {str(e)}")
            # ä½¿ç”¨ç®€åŒ–çš„ä¼°ç®—æ–¹æ³•
            if len(cluster_features) >= 2:
                # åªè®¡ç®—å‰ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ç›¸ä¼¼åº¦ä½œä¸ºä¼°ç®—
                from sklearn.metrics.pairwise import cosine_similarity
                similarity = cosine_similarity([cluster_features[0]], [cluster_features[1]])[0][0]
                return float(similarity)
            return 0.8
        except Exception as e:
            logger.warning(f"è®¡ç®—èšç±»å¹³å‡ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.8  # é»˜è®¤å€¼
    
    def _calculate_photo_cluster_similarity(self, photo_feature, cluster_features: List) -> float:
        """
        è®¡ç®—ç…§ç‰‡ä¸èšç±»ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
        
        :param photo_feature: ç…§ç‰‡çš„ç‰¹å¾å‘é‡ï¼ˆå¯ä»¥æ˜¯listæˆ–np.ndarrayï¼‰
        :param cluster_features: èšç±»å†…çš„ç‰¹å¾å‘é‡åˆ—è¡¨
        :return: ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        # ç¡®ä¿ä¾èµ–åº“å·²åŠ è½½
        _lazy_import_dependencies()
        
        if np is None:
            logger.warning("numpyæœªåŠ è½½ï¼Œæ— æ³•è®¡ç®—ç…§ç‰‡èšç±»ç›¸ä¼¼åº¦")
            return 0.8  # è¿”å›é»˜è®¤å€¼
        
        try:
            # ç¡®ä¿photo_featureæ˜¯numpyæ•°ç»„
            if not isinstance(photo_feature, np.ndarray):
                photo_feature = np.array(photo_feature)
            
            features_array = np.array(cluster_features)
            
            # è®¡ç®—èšç±»ä¸­å¿ƒï¼ˆå‡å€¼ï¼‰
            cluster_center = np.mean(features_array, axis=0)
            
            # è®¡ç®—ç…§ç‰‡ä¸èšç±»ä¸­å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity([photo_feature], [cluster_center])[0][0]
            
            return float(similarity)
        except Exception as e:
            logger.warning(f"è®¡ç®—ç…§ç‰‡èšç±»ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.8  # é»˜è®¤å€¼
    
    def _calculate_cluster_quality(self, photo_count: int, avg_similarity: float) -> str:
        """
        è®¡ç®—èšç±»è´¨é‡
        
        :param photo_count: ç…§ç‰‡æ•°é‡
        :param avg_similarity: å¹³å‡ç›¸ä¼¼åº¦
        :return: è´¨é‡ç­‰çº§ï¼ˆhigh/medium/lowï¼‰
        """
        if photo_count >= 5 and avg_similarity >= 0.8:
            return "high"
        elif photo_count >= 3 and avg_similarity >= 0.7:
            return "medium"
        else:
            return "low"

