"""
å›¾åƒç‰¹å¾æå–æœåŠ¡

åŸºäºResNet50æ¨¡å‹æå–å›¾åƒç‰¹å¾å‘é‡

## åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. å»¶è¿ŸåŠ è½½PyTorchå’ŒResNet50æ¨¡å‹
2. æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œåœ¨çº¿æ¨¡å‹
3. æ”¯æŒHEICæ ¼å¼ï¼ˆé€šè¿‡PILï¼‰
4. æå–2048ç»´ç‰¹å¾å‘é‡
5. è‡ªåŠ¨L2å½’ä¸€åŒ–

## ä¸å…¶ä»–ç‰ˆæœ¬çš„ä¸åŒç‚¹ï¼š
- å‚è€ƒäººè„¸è¯†åˆ«æœåŠ¡çš„æ¶æ„è®¾è®¡
- ä½¿ç”¨å¼‚æ­¥å¤„ç†æé«˜æ€§èƒ½
- æ”¯æŒæ‰¹é‡ç‰¹å¾æå–
"""

import asyncio
import json
import logging
from typing import Optional, List, Dict
from pathlib import Path
from datetime import datetime
import numpy as np

# å»¶è¿Ÿå¯¼å…¥é‡å‹åº“
torch = None
torchvision = None
PIL = None
Image = None
transforms = None
models = None
HEIC_SUPPORT = False
orjson = None  # å¿«é€ŸJSONåº“

def _lazy_import_dependencies():
    """å»¶è¿Ÿå¯¼å…¥é‡å‹åº“"""
    global torch, torchvision, PIL, Image, transforms, models, HEIC_SUPPORT, orjson
    
    if torch is None:
        try:
            import torch
            import torchvision
            from torchvision import models, transforms
            from PIL import Image
            import PIL
            logging.info("âœ… æˆåŠŸåŠ è½½å›¾åƒç‰¹å¾æå–ä¾èµ–åº“")
            
            # HEICæ”¯æŒ
            try:
                from pillow_heif import register_heif_opener
                register_heif_opener()
                HEIC_SUPPORT = True
            except ImportError:
                HEIC_SUPPORT = False
                logging.warning("pillow-heifæœªå®‰è£…ï¼ŒHEICæ ¼å¼æ”¯æŒå—é™")
                
        except ImportError as e:
            logging.error(f"å›¾åƒç‰¹å¾æå–ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    
    # å»¶è¿Ÿå¯¼å…¥orjsonï¼ˆå¿«é€ŸJSONåº“ï¼‰
    # æ³¨æ„ï¼šorjsonä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨å¯¼å…¥ï¼ˆåœ¨_fast_json_loads/_fast_json_dumpsä¸­ï¼‰
    # è¿™é‡Œä¸å¼ºåˆ¶å¯¼å…¥ï¼Œå…è®¸ç³»ç»Ÿåœ¨orjsonä¸å¯ç”¨æ—¶ç»§ç»­è¿è¡Œ

from app.core.config import settings
from app.db.session import get_db
from app.models.photo import Photo
from app.core.logging import get_logger
from sqlalchemy.orm import Session

logger = get_logger(__name__)


def _fast_json_loads(json_str: str) -> list:
    """
    å¿«é€ŸJSONè§£æï¼ˆä½¿ç”¨orjsonï¼Œå¦‚æœå¯ç”¨ï¼‰
    
    :param json_str: JSONå­—ç¬¦ä¸²
    :return: è§£æåçš„Pythonå¯¹è±¡
    """
    global orjson
    
    # å¦‚æœorjsonè¿˜æœªå¯¼å…¥ï¼Œå°è¯•å¯¼å…¥
    if orjson is None:
        try:
            import orjson
            logger.debug("æˆåŠŸåŠ è½½orjsonå¿«é€ŸJSONåº“")
        except ImportError:
            orjson = False  # æ ‡è®°ä¸ºä¸å¯ç”¨ï¼Œé¿å…é‡å¤å°è¯•
            return json.loads(json_str)
    
    # å¦‚æœorjsonå¯ç”¨ï¼Œä½¿ç”¨å®ƒ
    if orjson is not False:
        try:
            return orjson.loads(json_str)
        except Exception as e:
            logger.warning(f"orjsonè§£æå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†json: {str(e)}")
            return json.loads(json_str)
    else:
        return json.loads(json_str)


def _fast_json_dumps(obj) -> str:
    """
    å¿«é€ŸJSONåºåˆ—åŒ–ï¼ˆä½¿ç”¨orjsonï¼Œå¦‚æœå¯ç”¨ï¼‰
    
    :param obj: è¦åºåˆ—åŒ–çš„Pythonå¯¹è±¡
    :return: JSONå­—ç¬¦ä¸²
    """
    global orjson
    
    # å¦‚æœorjsonè¿˜æœªå¯¼å…¥ï¼Œå°è¯•å¯¼å…¥
    if orjson is None:
        try:
            import orjson
            logger.debug("æˆåŠŸåŠ è½½orjsonå¿«é€ŸJSONåº“")
        except ImportError:
            orjson = False  # æ ‡è®°ä¸ºä¸å¯ç”¨ï¼Œé¿å…é‡å¤å°è¯•
            return json.dumps(obj, separators=(',', ':'))
    
    # å¦‚æœorjsonå¯ç”¨ï¼Œä½¿ç”¨å®ƒ
    if orjson is not False:
        try:
            # orjsonè¿”å›bytesï¼Œéœ€è¦è§£ç ä¸ºå­—ç¬¦ä¸²
            return orjson.dumps(obj, option=orjson.OPT_SERIALIZE_NUMPY).decode('utf-8')
        except Exception as e:
            logger.warning(f"orjsonåºåˆ—åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†json: {str(e)}")
            return json.dumps(obj, separators=(',', ':'))
    else:
        return json.dumps(obj, separators=(',', ':'))


class ImageFeatureService:
    """å›¾åƒç‰¹å¾æå–æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å›¾åƒç‰¹å¾æå–æœåŠ¡"""
        self.model = None
        self.preprocess = None
        self.is_initialized = False
        self.config = settings.image_features
        # æ³¨æ„ï¼šä¸å†åœ¨ __init__ ä¸­å›ºå®š storage_baseï¼Œæ”¹ä¸ºåŠ¨æ€è¯»å–
    
    @property
    def storage_base(self) -> Path:
        """åŠ¨æ€è·å–å­˜å‚¨åŸºç¡€è·¯å¾„ï¼ˆæ¯æ¬¡ä½¿ç”¨æ—¶è¯»å–æœ€æ–°é…ç½®ï¼‰"""
        from app.core.config import get_settings
        return Path(get_settings().storage.base_path).resolve()
    
    def _get_full_path(self, image_path: str) -> Path:
        """
        æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        
        :param image_path: ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
        :return: å®Œæ•´è·¯å¾„
        """
        path = Path(image_path)
        if path.is_absolute():
            return path
        return self.storage_base / image_path
    
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–ResNet50æ¨¡å‹
        
        :return: æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        # å»¶è¿Ÿå¯¼å…¥ä¾èµ–
        _lazy_import_dependencies()
        
        try:
            if not torch:
                logger.error("PyTorchæœªå®‰è£…ï¼Œæ— æ³•å¯ç”¨å›¾åƒç‰¹å¾æå–")
                return False
            
            if not self.config.enabled:
                logger.info("å›¾åƒç‰¹å¾æå–åŠŸèƒ½å·²ç¦ç”¨")
                return False
            
            logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–ResNet50æ¨¡å‹...")
            
            # æ ¹æ®é…ç½®å†³å®šä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿˜æ˜¯åœ¨çº¿æ¨¡å‹
            if self.config.use_local_model:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
                models_base_path = Path(self.config.models_base_path).resolve()
                model_file_path = models_base_path / self.config.model_file
                logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_file_path}")
                
                if not model_file_path.exists():
                    logger.error(f"æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file_path}")
                    return False
                
                # åŠ è½½ResNet50æ¨¡å‹
                # å…ˆåˆ›å»ºå®Œæ•´çš„æ¨¡å‹ç»“æ„
                model = models.resnet50(weights=None)
                
                # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡
                state_dict = torch.load(str(model_file_path), map_location='cpu')
                model.load_state_dict(state_dict)
                logger.info("âœ… æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡")
            else:
                # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåœ¨çº¿ä¸‹è½½ï¼‰
                logger.info(f"ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {self.config.model}")
                model = models.resnet50(weights='IMAGENET1K_V2')
            
            # ç§»é™¤åˆ†ç±»å±‚ï¼Œåªä¿ç•™ç‰¹å¾æå–éƒ¨åˆ†
            # ç§»é™¤æœ€åä¸¤å±‚ï¼šavgpoolå’Œfc
            backbone = torch.nn.Sequential(*list(model.children())[:-2])
            
            # æ·»åŠ å…¨å±€å¹³å‡æ± åŒ–å±‚ï¼Œå°†ç‰¹å¾å›¾è½¬æ¢ä¸ºå‘é‡
            # ç»“æ„ï¼šbackbone -> AdaptiveAvgPool2d -> Flatten
            self.model = torch.nn.Sequential(
                backbone,
                torch.nn.AdaptiveAvgPool2d((1, 1)),  # å…¨å±€å¹³å‡æ± åŒ–ï¼š7x7 -> 1x1
                torch.nn.Flatten()  # å±•å¹³ï¼š1x1x2048 -> 2048
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # å®šä¹‰å›¾åƒé¢„å¤„ç†
            self.preprocess = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
            
            self.is_initialized = True
            logger.info("âœ… ResNet50æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå·²å°±ç»ª")
            return True
            
        except Exception as e:
            logger.error(f"ResNet50æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def extract_features(self, image_path: str) -> Optional[np.ndarray]:
        """
        æå–å•å¼ ç…§ç‰‡çš„ç‰¹å¾å‘é‡
        
        :param image_path: ç…§ç‰‡è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰
        :return: ç‰¹å¾å‘é‡ï¼ˆ2048ç»´ï¼Œå·²å½’ä¸€åŒ–ï¼‰ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.is_initialized:
            logger.error("æ¨¡å‹æœªåˆå§‹åŒ–")
            return None
        
        try:
            full_path = self._get_full_path(image_path)
            
            if not full_path.exists():
                logger.error(f"ç…§ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
                return None
            
            # åŠ è½½å›¾åƒï¼ˆæ”¯æŒHEICï¼‰
            if full_path.suffix.lower() in ['.heic', '.heif'] and HEIC_SUPPORT:
                from pillow_heif import read_heif
                heif_file = read_heif(full_path)
                image = Image.frombytes(
                    heif_file.mode,
                    heif_file.size,
                    heif_file.data,
                    "raw",
                    heif_file.mode,
                    heif_file.stride,
                )
            else:
                image = Image.open(full_path)
            
            # è½¬æ¢ä¸ºRGB
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # é¢„å¤„ç†å¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
            input_tensor = self.preprocess(image)
            input_batch = input_tensor.unsqueeze(0)
            
            # æå–ç‰¹å¾ï¼ˆæ¨¡å‹å·²åŒ…å«å…¨å±€å¹³å‡æ± åŒ–å’Œå±•å¹³ï¼‰
            with torch.no_grad():
                feature = self.model(input_batch)  # [1, 2048]
            
            # ç§»é™¤batchç»´åº¦å¹¶è½¬æ¢ä¸ºnumpy
            feature = feature.squeeze(0)  # [2048]
            feature_vector = feature.numpy()
            
            # L2å½’ä¸€åŒ–
            feature_vector = feature_vector / np.linalg.norm(feature_vector)
            
            return feature_vector
            
        except Exception as e:
            logger.error(f"æå–ç‰¹å¾å¤±è´¥ {image_path}: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def save_features_to_db(self, photo_id: int, features: np.ndarray, db_session: Session) -> bool:
        """
        ä¿å­˜ç‰¹å¾å‘é‡åˆ°æ•°æ®åº“
        
        :param photo_id: ç…§ç‰‡ID
        :param features: ç‰¹å¾å‘é‡ï¼ˆnumpyæ•°ç»„ï¼‰
        :param db_session: æ•°æ®åº“ä¼šè¯
        :return: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            photo = db_session.query(Photo).filter(Photo.id == photo_id).first()
            if not photo:
                logger.error(f"ç…§ç‰‡ {photo_id} ä¸å­˜åœ¨")
                return False
            
            # åºåˆ—åŒ–ä¸ºJSONï¼ˆä½¿ç”¨å¿«é€ŸJSONåº“ï¼‰
            features_json = _fast_json_dumps(features.tolist())
            
            # æ›´æ–°æ•°æ®åº“
            photo.image_features = features_json
            photo.image_features_extracted = True
            photo.image_features_extracted_at = datetime.now()
            
            db_session.commit()
            logger.debug(f"âœ… ç‰¹å¾å‘é‡å·²ä¿å­˜åˆ°æ•°æ®åº“ (photo_id: {photo_id})")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç‰¹å¾å‘é‡å¤±è´¥ {photo_id}: {str(e)}")
            db_session.rollback()
            import traceback
            traceback.print_exc()
            return False
    
    def batch_save_features_to_db(self, features_data: List[Dict], db_session: Session) -> int:
        """
        æ‰¹é‡ä¿å­˜ç‰¹å¾å‘é‡åˆ°æ•°æ®åº“
        
        :param features_data: ç‰¹å¾æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'photo_id': int, 'features': np.ndarray}
        :param db_session: æ•°æ®åº“ä¼šè¯
        :return: æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        if not features_data:
            return 0
        
        try:
            # è·å–æ‰€æœ‰ç…§ç‰‡ID
            photo_ids = [item['photo_id'] for item in features_data]
            photos = db_session.query(Photo).filter(Photo.id.in_(photo_ids)).all()
            photo_dict = {photo.id: photo for photo in photos}
            
            saved_count = 0
            current_time = datetime.now()
            
            for item in features_data:
                photo_id = item['photo_id']
                features = item['features']
                
                photo = photo_dict.get(photo_id)
                if not photo:
                    logger.warning(f"ç…§ç‰‡ {photo_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                    continue
                
                try:
                    # åºåˆ—åŒ–ä¸ºJSON
                    features_json = json.dumps(features.tolist(), separators=(',', ':'))
                    
                    # æ›´æ–°æ•°æ®åº“
                    photo.image_features = features_json
                    photo.image_features_extracted = True
                    photo.image_features_extracted_at = current_time
                    
                    saved_count += 1
                except Exception as e:
                    logger.error(f"ä¿å­˜ç…§ç‰‡ {photo_id} ç‰¹å¾å‘é‡å¤±è´¥: {str(e)}")
                    continue
            
            # æ‰¹é‡æäº¤
            db_session.commit()
            logger.info(f"âœ… æ‰¹é‡ä¿å­˜ {saved_count}/{len(features_data)} ä¸ªç‰¹å¾å‘é‡åˆ°æ•°æ®åº“")
            return saved_count
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜ç‰¹å¾å‘é‡å¤±è´¥: {str(e)}")
            db_session.rollback()
            import traceback
            traceback.print_exc()
            return 0
    
    def load_features_from_db(self, photo: Photo) -> Optional[np.ndarray]:
        """
        ä»æ•°æ®åº“åŠ è½½ç‰¹å¾å‘é‡
        
        :param photo: ç…§ç‰‡å¯¹è±¡
        :return: ç‰¹å¾å‘é‡ï¼ˆnumpyæ•°ç»„ï¼‰ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            if not photo.image_features:
                return None
            
            # ååºåˆ—åŒ–JSONï¼ˆä½¿ç”¨å¿«é€ŸJSONåº“ï¼‰
            features_list = _fast_json_loads(photo.image_features)
            features = np.array(features_list)
            
            return features
            
        except Exception as e:
            logger.error(f"åŠ è½½ç‰¹å¾å‘é‡å¤±è´¥ {photo.id}: {str(e)}")
            return None
    
    def calculate_cosine_similarity(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        :param features1: ç¬¬ä¸€ä¸ªç‰¹å¾å‘é‡
        :param features2: ç¬¬äºŒä¸ªç‰¹å¾å‘é‡
        :return: ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        try:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(features1, features2)
            norm1 = np.linalg.norm(features1)
            norm2 = np.linalg.norm(features2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0
    
    def find_similar_photos_by_features(
        self,
        db_session: Session,
        reference_photo_id: int,
        threshold: float = 0.7,
        limit: int = 20
    ) -> List[Dict]:
        """
        åŸºäºç‰¹å¾å‘é‡æœç´¢ç›¸ä¼¼ç…§ç‰‡ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        ä½¿ç”¨numpyå‘é‡åŒ–è®¡ç®—ï¼Œå¤§å¹…æå‡å¤§æ•°æ®é‡ä¸‹çš„æœç´¢æ€§èƒ½
        å¯¹äº40000å¼ ç…§ç‰‡ï¼Œæ€§èƒ½æå‡å¯è¾¾10-100å€
        
        :param db_session: æ•°æ®åº“ä¼šè¯
        :param reference_photo_id: å‚è€ƒç…§ç‰‡ID
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        :param limit: è¿”å›æ•°é‡é™åˆ¶
        :return: ç›¸ä¼¼ç…§ç‰‡åˆ—è¡¨
        """
        try:
            # è·å–å‚è€ƒç…§ç‰‡
            reference_photo = db_session.query(Photo).filter(Photo.id == reference_photo_id).first()
            if not reference_photo:
                logger.error(f"å‚è€ƒç…§ç‰‡ {reference_photo_id} ä¸å­˜åœ¨")
                return []
            
            # åŠ è½½å‚è€ƒç…§ç‰‡çš„ç‰¹å¾å‘é‡
            reference_features = self.load_features_from_db(reference_photo)
            if reference_features is None:
                logger.warning(f"å‚è€ƒç…§ç‰‡ {reference_photo_id} æ²¡æœ‰ç‰¹å¾å‘é‡")
                return []
            
            # ç¡®ä¿å‚è€ƒç‰¹å¾å‘é‡æ˜¯numpyæ•°ç»„ä¸”ä¸º1D
            reference_features = np.array(reference_features).flatten()
            
            # è·å–æ‰€æœ‰å·²æå–ç‰¹å¾çš„ç…§ç‰‡ï¼ˆæ’é™¤å‚è€ƒç…§ç‰‡ï¼‰
            # åªæŸ¥è¯¢å¿…è¦çš„å­—æ®µï¼Œå‡å°‘å†…å­˜å ç”¨
            photos_with_features = db_session.query(
                Photo.id,
                Photo.filename,
                Photo.original_path,
                Photo.thumbnail_path,
                Photo.image_features,
                Photo.taken_at,
                Photo.created_at
            ).filter(
                Photo.id != reference_photo_id,
                Photo.image_features_extracted == True,
                Photo.image_features.isnot(None)
            ).all()
            
            if not photos_with_features:
                return []
            
            # æ‰¹é‡åŠ è½½æ‰€æœ‰ç‰¹å¾å‘é‡åˆ°numpyçŸ©é˜µï¼ˆå‘é‡åŒ–ä¼˜åŒ– + å¿«é€ŸJSONè§£æï¼‰
            photo_ids = []
            photo_info = []  # å­˜å‚¨ç…§ç‰‡çš„å…¶ä»–ä¿¡æ¯
            feature_matrix = []
            
            # æ‰¹é‡è§£æJSONï¼ˆä½¿ç”¨å¿«é€ŸJSONåº“ï¼‰
            reference_dim = reference_features.shape[0]
            for photo in photos_with_features:
                try:
                    # è·³è¿‡ç©ºçš„ç‰¹å¾å‘é‡
                    if not photo.image_features:
                        continue
                    
                    # ä½¿ç”¨å¿«é€ŸJSONè§£æï¼ˆorjsonæ¯”æ ‡å‡†jsonå¿«3-5å€ï¼‰
                    features_list = _fast_json_loads(photo.image_features)
                    features = np.array(features_list, dtype=np.float32).flatten()
                    
                    # éªŒè¯ç‰¹å¾å‘é‡ç»´åº¦
                    if features.shape[0] != reference_dim:
                        logger.warning(f"ç…§ç‰‡ {photo.id} ç‰¹å¾å‘é‡ç»´åº¦ä¸åŒ¹é…: {features.shape[0]} vs {reference_dim}")
                        continue
                    
                    photo_ids.append(photo.id)
                    photo_info.append({
                        'filename': photo.filename,
                        'file_path': photo.original_path,
                        'thumbnail_path': photo.thumbnail_path,
                        'taken_at': photo.taken_at,
                        'created_at': photo.created_at
                    })
                    feature_matrix.append(features)
                    
                except Exception as e:
                    logger.warning(f"åŠ è½½ç…§ç‰‡ {photo.id} ç‰¹å¾å‘é‡å¤±è´¥: {str(e)}")
                    continue
            
            if not feature_matrix:
                return []
            
            # è½¬æ¢ä¸ºnumpyçŸ©é˜µï¼ˆN x feature_dimï¼‰
            # ä½¿ç”¨float32å‡å°‘å†…å­˜å ç”¨ï¼ˆ2048ç»´ç‰¹å¾å‘é‡ï¼Œfloat32è¶³å¤Ÿç²¾ç¡®ï¼‰
            feature_matrix = np.array(feature_matrix, dtype=np.float32)
            
            # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆä¸€æ¬¡æ€§è®¡ç®—ï¼Œé¿å…å¾ªç¯ï¼‰
            # å‚è€ƒå‘é‡å½’ä¸€åŒ–
            ref_norm = np.linalg.norm(reference_features)
            if ref_norm == 0:
                logger.warning("å‚è€ƒç‰¹å¾å‘é‡ä¸ºé›¶å‘é‡")
                return []
            
            # è®¡ç®—æ‰€æœ‰ç…§ç‰‡ç‰¹å¾å‘é‡çš„L2èŒƒæ•°
            feature_norms = np.linalg.norm(feature_matrix, axis=1)
            
            # é¿å…é™¤é›¶é”™è¯¯
            valid_mask = feature_norms > 0
            if not np.any(valid_mask):
                return []
            
            # è®¡ç®—ç‚¹ç§¯ï¼ˆçŸ©é˜µä¹˜æ³•ï¼šN x feature_dim @ feature_dim = Nï¼‰
            dot_products = np.dot(feature_matrix[valid_mask], reference_features)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–ï¼‰
            similarities = dot_products / (feature_norms[valid_mask] * ref_norm)
            
            # ç­›é€‰æ»¡è¶³é˜ˆå€¼çš„ç»“æœ
            threshold_mask = similarities >= threshold
            
            if not np.any(threshold_mask):
                return []
            
            # è·å–æ»¡è¶³æ¡ä»¶çš„ç´¢å¼•
            valid_indices = np.where(valid_mask)[0]
            threshold_indices = valid_indices[threshold_mask]
            threshold_similarities = similarities[threshold_mask]
            
            # ä½¿ç”¨numpyçš„argsortè¿›è¡Œæ’åºï¼ˆæ¯”Pythonæ’åºå¿«ï¼‰
            sorted_indices = np.argsort(threshold_similarities)[::-1]  # é™åºæ’åº
            
            # é™åˆ¶è¿”å›æ•°é‡
            result_indices = sorted_indices[:limit]
            
            # æ„å»ºç»“æœåˆ—è¡¨
            similar_photos = []
            for idx in result_indices:
                photo_idx = threshold_indices[idx]
                similar_photos.append({
                    'photo_id': photo_ids[photo_idx],
                    'filename': photo_info[photo_idx]['filename'],
                    'file_path': photo_info[photo_idx]['file_path'],
                    'thumbnail_path': photo_info[photo_idx]['thumbnail_path'],
                    'similarity': float(threshold_similarities[idx]),
                    'taken_at': photo_info[photo_idx]['taken_at'],
                    'created_at': photo_info[photo_idx]['created_at']
                })
            
            return similar_photos
            
        except Exception as e:
            logger.error(f"æœç´¢ç›¸ä¼¼ç…§ç‰‡å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return []


# å…¨å±€æœåŠ¡å®ä¾‹
image_feature_service = ImageFeatureService()

